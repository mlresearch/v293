---
title: Regularized Neural Ensemblers
openreview: uB4olDCuU2
abstract: Ensemble methods are known for enhancing the accuracy and robustness of
  machine learning models by combining multiple base learners. However, standard approaches
  like greedy or random ensembling often fall short, as they assume a constant weight
  across samples for the ensemble members. This can limit expressiveness and hinder
  performance when aggregating the ensemble predictions. In this study, we explore
  employing regularized neural networks as ensemble methods, emphasizing the significance
  of dynamic ensembling to leverage diverse model predictions adaptively. Motivated
  by the risk of learning low-diversity ensembles, we propose regularizing the ensembling
  model by randomly dropping base model predictions during the training. We demonstrate
  this approach provides lower bounds for the diversity within the ensemble, reducing
  overfitting and improving generalization capabilities. Our experiments showcase
  that the regularized neural ensemblers yield competitive results compared to strong
  baselines across several modalities such as computer vision, natural language processing,
  and tabular data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arango25a
month: 0
tex_title: Regularized Neural Ensemblers
firstpage: 8/1
lastpage: 33
page: 8/1-33
order: 8
cycles: false
bibtex_author: Arango, Sebastian Pineda and Janowski, Maciej and Purucker, Lennart
  and Zela, Arber and Hutter, Frank and Grabocka, Josif
author:
- given: Sebastian Pineda
  family: Arango
- given: Maciej
  family: Janowski
- given: Lennart
  family: Purucker
- given: Arber
  family: Zela
- given: Frank
  family: Hutter
- given: Josif
  family: Grabocka
date: 2025-11-11
address:
container-title: Proceedings of the Fourth International Conference on Automated Machine
  Learning
volume: '293'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 11
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v293/main/assets/arango25a/arango25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
