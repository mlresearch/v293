---
title: What Makes Freezing Layers in Deep Neural Networks Effective? A Linear Separability
  Perspective
openreview: DALK4KJTjX
abstract: Freezing layers in deep neural networks has been shown to enhance generalization
  and accelerate training, yet the underlying mechanisms remain unclear. This paper
  investigates the impact of frozen layers from the perspective of linear separability,
  examining how untrained, randomly initialized layers influence feature representations
  and model performance. Using multilayer perceptrons trained on MNIST, CIFAR-10,
  and CIFAR-100, we systematically analyze the effects freezing layers and network
  architecture. While prior work attributes the benefits of frozen layers to Cover’s
  theorem, which suggests that nonlinear transformations improve linear separability,
  we find that this explanation is insufficient. Instead, our results indicate that
  the observed improvements in generalization and convergence stem from other mechanisms.
  We hypothesize that freezing may have similar effects to other regularization techniques
  and that it may smooth the loss landscape to facilitate training. Furthermore, we
  identify key architectural factors—such as network overparameterization and use
  of skip connections—that modulate the effectiveness of frozen layers. These findings
  offer new insights into the conditions under which freezing layers can optimize
  deep learning performance, informing future work on neural architecture search.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: coil25a
month: 0
tex_title: What Makes Freezing Layers in Deep Neural Networks Effective? A Linear
  Separability Perspective
firstpage: 9/1
lastpage: 32
page: 9/1-32
order: 9
cycles: false
bibtex_author: Coil, Collin and Cheney, Nick
author:
- given: Collin
  family: Coil
- given: Nick
  family: Cheney
date: 2025-11-11
address:
container-title: Proceedings of the Fourth International Conference on Automated Machine
  Learning
volume: '293'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 11
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v293/main/assets/coil25a/coil25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
