---
title: Overtuning in Hyperparameter Optimization
openreview: ODD5YfFyfg
abstract: 'Hyperparameter optimization (HPO) aims to identify an optimal hyperparameter
  configuration (HPC) such that the resulting model generalizes well to unseen data.
  As the expected generalization error cannot be optimized directly, it is estimated
  with a resampling strategy, such as holdout or cross-validation. This approach implicitly
  assumes that minimizing the validation error leads to improved generalization. However,
  since validation error estimates are inherently stochastic and depend on the resampling
  strategy, a natural question arises: Can excessive optimization of the validation
  error lead to overfitting at the HPO level, akin to overfitting in model training
  based on empirical risk minimization? In this paper, we investigate this phenomenon,
  which we term overtuning, a form of overfitting specific to HPO. Despite its practical
  relevance, overtuning has received limited attention in the HPO and AutoML literature.
  We provide a formal definition of overtuning and distinguish it from related concepts
  such as meta-overfitting. We then conduct a large-scale reanalysis of HPO benchmark
  data to assess the prevalence and severity of overtuning. Our results show that
  overtuning is more common than previously assumed, typically mild but occasionally
  severe. In approximately 10% of cases, overtuning leads to the selection of a seemingly
  optimal HPC with worse generalization error than the default or first configuration
  tried. We further analyze how factors such as performance metric, resampling strategy,
  dataset size, learning algorithm, and HPO method affect overtuning and discuss mitigation
  strategies. Our results highlight the need to raise awareness of overtuning, particularly
  in the small-data regime, indicating that further mitigation strategies should be
  studied.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: schneider25a
month: 0
tex_title: Overtuning in Hyperparameter Optimization
firstpage: 17/1
lastpage: 43
page: 17/1-43
order: 17
cycles: false
bibtex_author: Schneider, Lennart and Bischl, Bernd and Feurer, Matthias
author:
- given: Lennart
  family: Schneider
- given: Bernd
  family: Bischl
- given: Matthias
  family: Feurer
date: 2025-11-11
address:
container-title: Proceedings of the Fourth International Conference on Automated Machine
  Learning
volume: '293'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 11
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v293/main/assets/schneider25a/schneider25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
